{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all the libraries needed\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import maxnorm\n",
    "from pandas import read_csv, DataFrame\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/home/jsahewal/MarchMadness/scripts\n",
      "time: 5.29 ms\n"
     ]
    }
   ],
   "source": [
    "# Initial setup and set random seed\n",
    "np.random.seed(42)\n",
    "%cd '/home/jsahewal/MarchMadness/scripts/'\n",
    "os.chdir('/home/jsahewal/MarchMadness/scripts/')\n",
    "data_inp_dir = '../input/'\n",
    "data_out_dir = '../output/'\n",
    "data_out_file = data_out_dir + 'keras_final.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "# Load all the dataframes\n",
    "seeds_df = pd.read_csv(data_inp_dir + 'TourneySeeds.csv')\n",
    "tour_compact_results_df = pd.read_csv(data_inp_dir + 'TourneyCompactResults.csv')\n",
    "tour_detailed_results_df = pd.read_csv(data_inp_dir + 'TourneyDetailedResults.csv')\n",
    "season_compact_results_df = pd.read_csv(data_inp_dir + 'RegularSeasonCompactResults.csv')\n",
    "season_detailed_results_df = pd.read_csv(data_inp_dir + 'RegularSeasonDetailedResults.csv')\n",
    "teams_df = pd.read_csv(data_inp_dir + 'Teams.csv')\n",
    "seasons_df = pd.read_csv(data_inp_dir + 'Seasons.csv')\n",
    "submission_df = pd.read_csv(data_inp_dir + 'SampleSubmission.csv')\n",
    "tour_slots_df = pd.read_csv(data_inp_dir + 'TourneySlots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45.8 ms\n"
     ]
    }
   ],
   "source": [
    "# Transform the tournament data, get the seed number for winning and losing team and remove unnecessary columns\n",
    "tour_merged_df = pd.merge(left=tour_compact_results_df, right=seeds_df, how='left', left_on=['Season', 'Wteam'], right_on=['Season', 'Team'])\n",
    "tour_merged_df.rename(columns={'Seed' : 'Wseed', 'Team' : 'W_team'}, inplace=True)\n",
    "tour_merged_df = pd.merge(left=tour_merged_df, right=seeds_df, how='left', left_on=['Season', 'Lteam'], right_on=['Season', 'Team'])\n",
    "tour_merged_df.rename(columns={'Seed' : 'Lseed', 'Team' : 'L_team'}, inplace=True)\n",
    "tour_cleaned_df = tour_merged_df[['Season', 'Wteam', 'Lteam', 'Wseed', 'Lseed']].copy()\n",
    "tour_cleaned_df['Wseed'] = tour_cleaned_df['Wseed'].map(lambda x: int(x[1:3]))\n",
    "tour_cleaned_df['Lseed'] = tour_cleaned_df['Lseed'].map(lambda x: int(x[1:3]))\n",
    "tour_cleaned_df['Seed_diff'] = tour_cleaned_df['Wseed'] - tour_cleaned_df['Lseed']\n",
    "tour_winning_df = pd.DataFrame()\n",
    "tour_winning_df[['Team1', 'Team2', 'Seed_diff']] = tour_cleaned_df[['Wteam', 'Lteam', 'Seed_diff']].copy()\n",
    "tour_winning_df['result'] = 1\n",
    "tour_losing_df = pd.DataFrame()\n",
    "tour_losing_df[['Team1', 'Team2', 'Seed_diff']] = tour_cleaned_df[['Lteam', 'Wteam', 'Seed_diff']].copy()\n",
    "tour_losing_df['Seed_diff'] = -tour_losing_df['Seed_diff']\n",
    "tour_losing_df['result'] = 0\n",
    "tour_train_final_df = pd.concat((tour_winning_df, tour_losing_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 419 ms\n"
     ]
    }
   ],
   "source": [
    "# Transform the season data, get the seed number for winning and losing team and remove unnecessary columns\n",
    "season_merged_df = pd.merge(left=season_compact_results_df, right=seeds_df, how='left', left_on=['Season', 'Wteam'], right_on=['Season', 'Team'])\n",
    "season_merged_df.rename(columns={'Seed' : 'Wseed', 'Team' : 'W_team'}, inplace=True)\n",
    "season_merged_df = pd.merge(left=season_merged_df, right=seeds_df, how='left', left_on=['Season', 'Lteam'], right_on=['Season', 'Team'])\n",
    "season_merged_df.rename(columns={'Seed' : 'Lseed', 'Team' : 'L_team'}, inplace=True)\n",
    "season_cleaned_df = season_merged_df[['Season', 'Wteam', 'Lteam', 'Wseed', 'Lseed']].copy()\n",
    "season_cleaned_df.fillna('T17', inplace=True)\n",
    "season_cleaned_df['Wseed'] = season_cleaned_df['Wseed'].map(lambda x: int(x[1:3]))\n",
    "season_cleaned_df['Lseed'] = season_cleaned_df['Lseed'].map(lambda x: int(x[1:3]))\n",
    "season_cleaned_df['Seed_diff'] = season_cleaned_df['Wseed'] - season_cleaned_df['Lseed']\n",
    "season_winning_df = pd.DataFrame()\n",
    "season_winning_df[['Team1', 'Team2', 'Seed_diff']] = season_cleaned_df[['Wteam', 'Lteam', 'Seed_diff']].copy()\n",
    "season_winning_df['result'] = 1\n",
    "season_losing_df = pd.DataFrame()\n",
    "season_losing_df[['Team1', 'Team2', 'Seed_diff']] = season_cleaned_df[['Lteam', 'Wteam', 'Seed_diff']].copy()\n",
    "season_losing_df['Seed_diff'] = -season_losing_df['Seed_diff']\n",
    "season_losing_df['result'] = 0\n",
    "season_train_final_df = pd.concat((season_winning_df, season_losing_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "# Create final input dataframe\n",
    "input_train_final_df = pd.concat((season_train_final_df, tour_train_final_df), ignore_index=True)\n",
    "team_dict = {t: i for i, t in enumerate(input_train_final_df.Team1.unique())}\n",
    "input_train_final_df['Team1'] = input_train_final_df['Team1'].apply(lambda x: team_dict[x])\n",
    "input_train_final_df['Team2'] = input_train_final_df['Team2'].apply(lambda x: team_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.3 ms\n"
     ]
    }
   ],
   "source": [
    "X1 = scale(input_train_final_df[['Team1', 'Team2']])\n",
    "X2 = scale(input_train_final_df[['Team2', 'Seed_diff']])\n",
    "X3 = scale(input_train_final_df[['Team2', 'Seed_diff']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 2)             6           dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             6           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNormal(None, 2)             4           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             6           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNormal(None, 2)             4           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             6           dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNormal(None, 2)             4           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 2)             6           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 2)             6           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNormal(None, 2)             4           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 2)             6           dense_input_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNormal(None, 2)             4           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 2)             6           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNormal(None, 2)             4           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 2)             6           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             3           merge_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 81\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "26s - loss: 0.6153\n",
      "Epoch 2/10\n",
      "26s - loss: 0.6131\n",
      "Epoch 3/10\n",
      "26s - loss: 0.6131\n",
      "Epoch 4/10\n",
      "26s - loss: 0.6129\n",
      "Epoch 5/10\n",
      "27s - loss: 0.6128\n",
      "Epoch 6/10\n",
      "27s - loss: 0.6125\n",
      "Epoch 7/10\n",
      "26s - loss: 0.6125\n",
      "Epoch 8/10\n",
      "26s - loss: 0.6126\n",
      "Epoch 9/10\n",
      "26s - loss: 0.6125\n",
      "Epoch 10/10\n",
      "26s - loss: 0.6125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.34172741],\n",
       "       [ 0.53873098],\n",
       "       [ 0.82313931],\n",
       "       ..., \n",
       "       [ 0.15761016],\n",
       "       [ 0.47347084],\n",
       "       [ 0.57613689]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning Model\n",
    "branch1 = Sequential()\n",
    "branch1.add(Dense(X1.shape[1], input_shape =  (X1.shape[1],), init = 'normal', activation = 'relu'))\n",
    "branch1.add(Dense(X1.shape[1], init = 'normal', activation = 'relu', W_constraint = maxnorm(5)))\n",
    "branch1.add(BatchNormalization())\n",
    "branch1.add(Dense(X1.shape[1], init = 'normal', activation = 'relu', W_constraint = maxnorm(5)))\n",
    "branch1.add(BatchNormalization())\n",
    "\n",
    "branch2 = Sequential()\n",
    "branch2.add(Dense(X2.shape[1], input_shape =  (X2.shape[1],), init = 'normal', activation = 'relu'))\n",
    "branch2.add(BatchNormalization())\n",
    "branch2.add(Dense(X2.shape[1], init = 'normal', activation = 'relu', W_constraint = maxnorm(5)))\n",
    "branch2.add(Dense(X2.shape[1], init = 'normal', activation = 'relu', W_constraint = maxnorm(5)))\n",
    "branch2.add(BatchNormalization())\n",
    "\n",
    "branch3 = Sequential()\n",
    "branch3.add(Dense(X3.shape[1], input_shape =  (X3.shape[1],), init = 'normal', activation = 'relu'))\n",
    "branch3.add(BatchNormalization())\n",
    "branch3.add(Dense(X3.shape[1], init = 'normal', activation = 'relu', W_constraint = maxnorm(5)))\n",
    "branch3.add(BatchNormalization())\n",
    "branch3.add(Dense(X3.shape[1], init = 'normal', activation = 'relu', W_constraint = maxnorm(5)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([branch1, branch2, branch3], mode = 'sum'))\n",
    "model.add(Dense(1, init = 'normal', activation = 'sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0, nesterov = False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "model.compile(Adam(0.001), loss=\"binary_crossentropy\")\n",
    "\n",
    "history = model.fit([X1, X2, X3], input_train_final_df['result'].values, batch_size = 64, nb_epoch = 10, verbose = 2)\n",
    "model.predict([X1, X2, X3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "# Process test data and create predictions for it\n",
    "test_df = pd.DataFrame()\n",
    "test_df['Season'] = submission_df['Id'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "test_df['Team1'] = submission_df['Id'].apply(lambda x: int(x.split(\"_\")[1]))\n",
    "test_df['Team2'] = submission_df['Id'].apply(lambda x: int(x.split(\"_\")[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.6 ms\n"
     ]
    }
   ],
   "source": [
    "test_merged_df = pd.merge(left=test_df, right=seeds_df, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'Team'])\n",
    "test_merged_df.rename(columns={'Seed' : 'Seed1', 'Team' : 'team_1'}, inplace=True)\n",
    "test_merged_df = pd.merge(left=test_merged_df, right=seeds_df, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'Team'])\n",
    "test_merged_df.rename(columns={'Seed' : 'Seed2', 'Team' : 'team_2'}, inplace=True)\n",
    "test_cleaned_df = test_merged_df[['Season', 'Team1', 'Team2', 'Seed1', 'Seed2']].copy()\n",
    "test_cleaned_df['Seed1'] = test_cleaned_df['Seed1'].map(lambda x: int(x[1:3]))\n",
    "test_cleaned_df['Seed2'] = test_cleaned_df['Seed2'].map(lambda x: int(x[1:3]))\n",
    "test_cleaned_df['Seed_diff'] = test_cleaned_df['Seed1'] - test_cleaned_df['Seed2']\n",
    "test_cleaned_df['Team1'] = test_cleaned_df['Team1'].apply(lambda x: team_dict[x])\n",
    "test_cleaned_df['Team2'] = test_cleaned_df['Team2'].apply(lambda x: team_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.87 ms\n"
     ]
    }
   ],
   "source": [
    "Z1 = scale(test_cleaned_df[['Team1', 'Team2']])\n",
    "Z2 = scale(test_cleaned_df[['Team2', 'Seed_diff']])\n",
    "Z3 = scale(test_cleaned_df[['Team2', 'Seed_diff']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 91.5 ms\n"
     ]
    }
   ],
   "source": [
    "test_pred_prob = model.predict([Z1, Z2, Z3])\n",
    "final_output = pd.DataFrame()\n",
    "final_output['Id'] = submission_df['Id']\n",
    "final_output['Pred'] = test_pred_prob\n",
    "final_output.to_csv(data_out_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
